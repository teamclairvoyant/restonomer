<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-persistence/gcp_bigquery_persistence">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">BigQuery | Restonomer</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://teamclairvoyant.github.io/restonomer/docs/persistence/gcp_bigquery_persistence"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="BigQuery | Restonomer"><meta data-rh="true" name="description" content="User can use BigQuery persistence to write/persist spark dataframe to google cloud BigQuery table."><meta data-rh="true" property="og:description" content="User can use BigQuery persistence to write/persist spark dataframe to google cloud BigQuery table."><link data-rh="true" rel="icon" href="/restonomer/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://teamclairvoyant.github.io/restonomer/docs/persistence/gcp_bigquery_persistence"><link data-rh="true" rel="alternate" href="https://teamclairvoyant.github.io/restonomer/docs/persistence/gcp_bigquery_persistence" hreflang="en"><link data-rh="true" rel="alternate" href="https://teamclairvoyant.github.io/restonomer/docs/persistence/gcp_bigquery_persistence" hreflang="x-default"><link rel="stylesheet" href="/restonomer/assets/css/styles.eedc1409.css">
<link rel="preload" href="/restonomer/assets/js/runtime~main.70a8c070.js" as="script">
<link rel="preload" href="/restonomer/assets/js/main.284a477a.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/restonomer/"><b class="navbar__title text--truncate">Restonomer</b></a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/teamclairvoyant/restonomer" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/restonomer/docs/restonomer_intro">Restonomer</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/restonomer/docs/getting_started/add_sbt_dependency">Getting Started</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/restonomer/docs/restonomer_context/restonomer_context_directory">Restonomer Context</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/restonomer/docs/running_checkpoints/run_single_checkpoint">Running Checkpoints</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/restonomer/docs/config_classes/checkpoint_config">Config Classes</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/restonomer/docs/token_request/token_request_intro">Token Request</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/restonomer/docs/authentication/api_key_authentication">Authentication</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/restonomer/docs/request_body/form_data_body">Request Body</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/restonomer/docs/restonomer_retry">Retry Mechanism</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/restonomer/docs/response_body/text/csv_text">Response Body</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/restonomer/docs/pagination/cursor_based_pagination">Pagination</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/restonomer/docs/transformation/add_column">Transformations</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/restonomer/docs/persistence/file_system_persistence">Persistence</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/restonomer/docs/persistence/file_system_persistence">LocalFileSystem</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/restonomer/docs/persistence/gcp_bigquery_persistence">BigQuery</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/restonomer/docs/persistence/gcs_bucket_persistence">GCSBucket</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/restonomer/docs/persistence/redshift_persistence">Redshift</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/restonomer/docs/persistence/s3_bucket_persistence">S3Bucket</a></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/restonomer/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Persistence</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">BigQuery</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>BigQuery</h1><p>User can use BigQuery persistence to write/persist spark dataframe to google cloud BigQuery table.</p><p>There are two ways to write the dataframe to BigQuery table:</p><ul><li>Direct Write</li><li>Indirect Write</li></ul><p>You can read about the difference between these two
approaches <a href="https://github.com/GoogleCloudDataproc/spark-bigquery-connector#writing-data-to-bigquery" target="_blank" rel="noopener noreferrer">here</a>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="direct-write">Direct Write<a href="#direct-write" class="hash-link" aria-label="Direct link to Direct Write" title="Direct link to Direct Write">​</a></h2><p>User can configure the BigQuery persistence using direct write approach in the below manner:</p><div class="language-hocon codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-hocon codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">persistence = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    type = &quot;BigQuery&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    service-account-credentials-file = &quot;/Users/xyz/Downloads/creds-file.json&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    table = &quot;project-name:dataset-name.table-name&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    writer-type = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        type = &quot;DirectBigQueryWriterType&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        http-connect-timeout = 5000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Apart from <code>http-connect-timeout</code>, user can configure below other parameters in the writer-type:</p><table><thead><tr><th align="left">Parameter Name</th><th align="center">Default Value</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left">big-query-table-label</td><td align="center">Empty List</td><td align="left">Can be used to add labels to the table while writing to a table. Multiple labels can be set.</td></tr><tr><td align="left">clustered-fields</td><td align="center">None</td><td align="left">A string of non-repeated, top-level columns separated by a comma.</td></tr><tr><td align="left">create-disposition</td><td align="center">CREATE_IF_NEEDED</td><td align="left">Specifies whether the job is allowed to create new tables. The permitted values are:<br>CREATE_IF_NEEDED - Configures the job to create the table if it does not exist<br>CREATE_NEVER - Configures the job to fail if the table does not exist <br>This option takes place only in case Spark has decided to write data to the table based on the SaveMode.</td></tr><tr><td align="left">datetime-zone-id</td><td align="center">UTC</td><td align="left">The time zone ID used to convert BigQuery&#x27;s DATETIME into Spark&#x27;s Timestamp, and vice versa.<br>The value should be a legal time zone name, that appears is accepted by Java&#x27;s java.time.ZoneId.</td></tr><tr><td align="left">destination-table-kms-key-name</td><td align="center">None</td><td align="left">Describes the Cloud KMS encryption key that will be used to protect the destination BigQuery table. The BigQuery Service Account associated with your project requires access to this encryption key. For further Information about using CMEK with BigQuery see <a href="https://cloud.google.com/bigquery/docs/customer-managed-encryption#key_resource_id" target="_blank" rel="noopener noreferrer">here</a>.<br>The table will be encrypted by the key only if it created by the connector. A pre-existing unencrypted table won&#x27;t be encrypted just by setting this option.</td></tr><tr><td align="left">enable-list-inference</td><td align="center">false</td><td align="left">Indicates whether to use schema inference specifically when the mode is Parquet.</td></tr><tr><td align="left">enable-mode-check-for-schema-fields</td><td align="center">true</td><td align="left">Checks the mode of every field in the destination schema to be equal to the mode in corresponding source field schema, during DIRECT write.</td></tr><tr><td align="left">http-connect-timeout</td><td align="center">6000</td><td align="left">The timeout in milliseconds to establish a connection with BigQuery. Can be alternatively set in the Spark configuration (spark.conf.set(&quot;httpConnectTimeout&quot;, ...)) or in Hadoop Configuration (fs.gs.http.connect-timeout).</td></tr><tr><td align="left">http-max-retry</td><td align="center">10</td><td align="left">The maximum number of retries for the low-level HTTP requests to BigQuery. Can be alternatively set in the Spark configuration (spark.conf.set(&quot;httpMaxRetry&quot;, ...)) or in Hadoop Configuration (fs.gs.http.max.retry).</td></tr><tr><td align="left">proxy-address</td><td align="center">None</td><td align="left">Address of the proxy server. The proxy must be an HTTP proxy, and the address should be in the <code>host:port</code> format. Can be alternatively set in the Spark configuration (spark.conf.set(...)) or in Hadoop Configuration (fs.gs.proxy.address).<br>(Optional. Required only if connecting to GCP via proxy.)</td></tr><tr><td align="left">proxy-username</td><td align="center">None</td><td align="left">The userName used to connect to the proxy. Can be alternatively set in the Spark configuration (spark.conf.set(...)) or in Hadoop Configuration (fs.gs.proxy.username).</td></tr><tr><td align="left">proxy-password</td><td align="center">None</td><td align="left">The password used to connect to the proxy. Can be alternatively set in the Spark configuration (spark.conf.set(...)) or in Hadoop Configuration (fs.gs.proxy.password).</td></tr><tr><td align="left">query-job-priority</td><td align="center">INTERACTIVE</td><td align="left">Priority levels set for the job while reading data from BigQuery query. The permitted values are:<br>BATCH - Query is queued and started as soon as idle resources are available, usually within a few minutes. If the query hasn&#x27;t started within 3 hours, its priority is changed to INTERACTIVE.<br>INTERACTIVE - Query is executed as soon as possible and counts towards the concurrent rate limit and the daily rate limit.<br>For WRITE, this option will be effective when DIRECT write is used with OVERWRITE mode, where the connector overwrites the destination table using MERGE statement.</td></tr><tr><td align="left">write-at-least-once</td><td align="center">false</td><td align="left">Guarantees that data is written to BigQuery at least once. This is a lesser guarantee than exactly once. This is suitable for streaming scenarios in which data is continuously being written in small batches.<br>Supported only by the <code>DIRECT</code> write method and mode is NOT <code>Overwrite</code>.</td></tr></tbody></table><h2 class="anchor anchorWithStickyNavbar_LWe7" id="indirect-write">Indirect Write<a href="#indirect-write" class="hash-link" aria-label="Direct link to Indirect Write" title="Direct link to Indirect Write">​</a></h2><p>User can configure the BigQuery persistence using indirect write approach in the below manner:</p><div class="language-hocon codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-hocon codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">persistence = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    type = &quot;BigQuery&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    service-account-credentials-file = &quot;/Users/xyz/Downloads/creds-file.json&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    table = &quot;project-name:dataset-name.table-name&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    writer-type = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        type = &quot;IndirectBigQueryWriterType&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        temporary-gcs-bucket = &quot;temp-bucket&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Apart from <code>temporary-gcs-bucket</code>, user can configure below other parameters in the writer-type:</p><table><thead><tr><th align="left">Parameter Name</th><th align="center">Default Value</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left">allow-field-addition</td><td align="center">false</td><td align="left">Adds the ALLOW_FIELD_ADDITION SchemaUpdateOption to the BigQuery LoadJob. Allowed values are true and false.</td></tr><tr><td align="left">allow-field-relaxation</td><td align="center">false</td><td align="left">Adds the ALLOW_FIELD_RELAXATION SchemaUpdateOption to the BigQuery LoadJob. Allowed values are true and false.</td></tr><tr><td align="left">big-query-table-label</td><td align="center">Empty List</td><td align="left">Can be used to add labels to the table while writing to a table. Multiple labels can be set.</td></tr><tr><td align="left">clustered-fields</td><td align="center">None</td><td align="left">A string of non-repeated, top-level columns separated by a comma.</td></tr><tr><td align="left">create-disposition</td><td align="center">CREATE_IF_NEEDED</td><td align="left">Specifies whether the job is allowed to create new tables. The permitted values are:<br>CREATE_IF_NEEDED - Configures the job to create the table if it does not exist<br>CREATE_NEVER - Configures the job to fail if the table does not exist <br>This option takes place only in case Spark has decided to write data to the table based on the SaveMode.</td></tr><tr><td align="left">datetime-zone-id</td><td align="center">UTC</td><td align="left">The time zone ID used to convert BigQuery&#x27;s DATETIME into Spark&#x27;s Timestamp, and vice versa.<br>The value should be a legal time zone name, that appears is accepted by Java&#x27;s java.time.ZoneId.</td></tr><tr><td align="left">date-partition</td><td align="center">None</td><td align="left">The date partition the data is going to be written to. Should be a date string given in the format YYYYMMDD. Can be used to overwrite the data of a single partition.<br>Can also be used with different partition types like:<br>HOUR: YYYYMMDDHH<br>MONTH: YYYYMM<br>YEAR: YYYY</td></tr><tr><td align="left">destination-table-kms-key-name</td><td align="center">None</td><td align="left">Describes the Cloud KMS encryption key that will be used to protect the destination BigQuery table. The BigQuery Service Account associated with your project requires access to this encryption key. for further Information about using CMEK with BigQuery see <a href="https://cloud.google.com/bigquery/docs/customer-managed-encryption#key_resource_id" target="_blank" rel="noopener noreferrer">here</a>.<br>The table will be encrypted by the key only if it created by the connector. A pre-existing unencrypted table won&#x27;t be encrypted just by setting this option.</td></tr><tr><td align="left">enable-list-inference</td><td align="center">false</td><td align="left">Indicates whether to use schema inference specifically when the mode is Parquet.</td></tr><tr><td align="left">http-connect-timeout</td><td align="center">6000</td><td align="left">The timeout in milliseconds to establish a connection with BigQuery. Can be alternatively set in the Spark configuration (spark.conf.set(&quot;httpConnectTimeout&quot;, ...)) or in Hadoop Configuration (fs.gs.http.connect-timeout).</td></tr><tr><td align="left">http-max-retry</td><td align="center">10</td><td align="left">The maximum number of retries for the low-level HTTP requests to BigQuery. Can be alternatively set in the Spark configuration (spark.conf.set(&quot;httpMaxRetry&quot;, ...)) or in Hadoop Configuration (fs.gs.http.max.retry).</td></tr><tr><td align="left">intermediate-format</td><td align="center">parquet</td><td align="left">The format of the data before it is loaded to BigQuery, values can be either &quot;parquet&quot;,&quot;orc&quot; or &quot;avro&quot;. In order to use the Avro format, the spark-avro package must be added in runtime.</td></tr><tr><td align="left">partition-expiration-ms</td><td align="center">None</td><td align="left">Number of milliseconds for which to keep the storage for partitions in the table. The storage in a partition will have an expiration time of its partition time plus this value.</td></tr><tr><td align="left">partition-field</td><td align="center">None</td><td align="left">If field is specified together with <code>partition-type</code>, the table is partitioned by this field. The field must be a top-level TIMESTAMP or DATE field. Its mode must be NULLABLE or REQUIRED. If the option is not set for a partitioned table, then the table will be partitioned by pseudo column, referenced via either &#x27;_PARTITIONTIME&#x27; as TIMESTAMP type, or &#x27;_PARTITIONDATE&#x27; as DATE type.</td></tr><tr><td align="left">partition-type</td><td align="center">None</td><td align="left">Supported types are: HOUR, DAY, MONTH, YEAR. This option is mandatory for a target table to be partitioned. (Optional. Defaults to DAY if PartitionField is specified).</td></tr><tr><td align="left">persistent-gcs-bucket</td><td align="center">None</td><td align="left">The GCS bucket that holds the data before it is loaded to BigQuery. If informed, the data won&#x27;t be deleted after write data into BigQuery.</td></tr><tr><td align="left">persistent-gcs-path</td><td align="center">None</td><td align="left">The GCS path that holds the data before it is loaded to BigQuery. Used only with persistent-gcs-bucket.</td></tr><tr><td align="left">proxy-address</td><td align="center">None</td><td align="left">Address of the proxy server. The proxy must be an HTTP proxy and address should be in the <code>host:port</code> format. Can be alternatively set in the Spark configuration (spark.conf.set(...)) or in Hadoop Configuration (fs.gs.proxy.address).<br>(Optional. Required only if connecting to GCP via proxy.)</td></tr><tr><td align="left">proxy-username</td><td align="center">None</td><td align="left">The userName used to connect to the proxy. Can be alternatively set in the Spark configuration (spark.conf.set(...)) or in Hadoop Configuration (fs.gs.proxy.username).</td></tr><tr><td align="left">proxy-password</td><td align="center">None</td><td align="left">The password used to connect to the proxy. Can be alternatively set in the Spark configuration (spark.conf.set(...)) or in Hadoop Configuration (fs.gs.proxy.password).</td></tr><tr><td align="left">temporary-gcs-bucket</td><td align="center">None</td><td align="left">The GCS bucket that temporarily holds the data before it is loaded to BigQuery. Required unless set in the Spark configuration (spark.conf.set(...)).</td></tr><tr><td align="left">use-avro-logical-types</td><td align="center">false</td><td align="left">When loading from Avro (<code>.option(&quot;intermediateFormat&quot;, &quot;avro&quot;)</code>), BigQuery uses the underlying Avro types instead of the logical types <!-- -->[by default]<!-- -->.  Supplying this option converts Avro logical types to their corresponding BigQuery data types.</td></tr></tbody></table><p>Irrespective of the direct or indirect write approach, the <code>BigQuery</code> persistence needs below arguments from the user:</p><table><thead><tr><th align="left">Parameter Name</th><th align="center">Mandatory</th><th align="center">Default Value</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left">table</td><td align="center">Yes</td><td align="center">None</td><td align="left">The name of big query table in the format <code>[[project:]dataset.]table</code> where dataframe needs to be persisted.</td></tr><tr><td align="left">service-account-credentials-file</td><td align="center">No</td><td align="center">None</td><td align="left">The filepath of the GCP service account credentials.</td></tr><tr><td align="left">dataset</td><td align="center">No</td><td align="center">None</td><td align="left">The dataset containing the table. If you are providing fully qualified name in <code>table</code> parameter, then you can ignore this option.</td></tr><tr><td align="left">project</td><td align="center">No</td><td align="center">None</td><td align="left">The Google Cloud Project ID of the table.<br>(Optional. Defaults to the project of the Service Account being used)</td></tr><tr><td align="left">parent-project</td><td align="center">No</td><td align="center">None</td><td align="left">The Google Cloud Project ID of the table to bill for the export.<br>(Optional. Defaults to the project of the Service Account being used).</td></tr><tr><td align="left">save-mode</td><td align="center">No</td><td align="center">ErrorIfExists</td><td align="left">This is used to specify the expected behavior of saving a DataFrame to a data source.<br> Expected values are (append, overwrite, errorifexists, ignore)</td></tr><tr><td align="left">writer-type</td><td align="center">Yes</td><td align="center">None</td><td align="left">The instance of direct or indirect big query writer type.</td></tr></tbody></table><p>Also, note that for writing to the BigQuery it is necessary to have below privileges to the user:</p><table><thead><tr><th align="left">Role Name</th><th align="left">Purpose</th></tr></thead><tbody><tr><td align="left">roles/bigquery.dataEditor</td><td align="left">Access BigQuery Tables</td></tr><tr><td align="left">roles/bigquery.jobUser</td><td align="left">Create and query BigQuery tables</td></tr><tr><td align="left">roles/storage.objectViewer</td><td align="left">To list and read GCS contents</td></tr><tr><td align="left">roles/storage.objectCreator</td><td align="left">To create folders in GCS</td></tr></tbody></table></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/restonomer/docs/persistence/file_system_persistence"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">LocalFileSystem</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/restonomer/docs/persistence/gcs_bucket_persistence"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">GCSBucket</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#direct-write" class="table-of-contents__link toc-highlight">Direct Write</a></li><li><a href="#indirect-write" class="table-of-contents__link toc-highlight">Indirect Write</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024, Clairvoyant (EXL Company)</div></div></div></footer></div>
<script src="/restonomer/assets/js/runtime~main.70a8c070.js"></script>
<script src="/restonomer/assets/js/main.284a477a.js"></script>
</body>
</html>